# This EXPERIMENTAL configuration is for ESPnet2 to finetune
# Conformer FastSpeech2 + HiFiGAN vocoder jointly. To run
# this config, you need to specify "--tts_task gan_tts"
# option for tts.sh at least and use 22050 hz audio as the
# training data (mainly tested on LJspeech).
# This configuration tested on 4 GPUs with 12GB GPU memory.
# It takes around less than 1 week to finish the training but
# 100k iters model should generate reasonable results.

# YOU NEED TO MODIFY THE "*_params" AND "init_param" SECTIONS
# IF YOU WANT TO USE YOUR OWN PRETRAINED MODLES.

##########################################################
#                  TTS MODEL SETTING                     #
##########################################################
tts: joint_text2wav
tts_conf:
    # copied from pretrained model's config.yaml
    text2mel_type: transformer # model architecture
    text2mel_params:           # keyword arguments for the selected model
        embed_dim: 0           # embedding dimension in encoder prenet
        eprenet_conv_layers: 0 # number of conv layers in encoder prenet
                            # if set to 0, no encoder prenet will be used
        eprenet_conv_filts: 0  # filter size of conv layers in encoder prenet
        eprenet_conv_chans: 0  # number of channels of conv layers in encoder prenet
        dprenet_layers: 2      # number of layers in decoder prenet
        dprenet_units: 256     # number of units in decoder prenet
        adim: 512              # attention dimension
        aheads: 4              # number of attention heads
        elayers: 3             # number of encoder layers
        eunits: 1024           # number of encoder ff units
        dlayers: 3             # number of decoder layers
        dunits: 1024           # number of decoder ff units
        positionwise_layer_type: conv1d  # type of position-wise layer
        positionwise_conv_kernel_size: 1 # kernel size of position wise conv layer
        postnet_layers: 5                # number of layers of postnset
        postnet_filts: 5                 # filter size of conv layers in postnet
        postnet_chans: 256               # number of channels of conv layers in postnet
        use_masking: True                # whether to apply masking for padded part in loss calculation
        bce_pos_weight: 5.0              # weight of positive sample in binary cross entropy calculation
        use_scaled_pos_enc: True         # whether to use scaled positional encoding
        encoder_normalize_before: True   # whether to perform layer normalization before the input
        decoder_normalize_before: True   # whether to perform layer normalization before the input
        reduction_factor: 4              # reduction factor
        init_type: xavier_uniform        # initialization type
        init_enc_alpha: 1.0              # initial value of alpha of encoder scaled position encoding
        init_dec_alpha: 1.0              # initial value of alpha of decoder scaled position encoding
        eprenet_dropout_rate: 0.0        # dropout rate for encoder prenet
        dprenet_dropout_rate: 0.99       # dropout rate for decoder prenet
        postnet_dropout_rate: 0.5        # dropout rate for postnet
        transformer_enc_dropout_rate: 0.1                # dropout rate for transformer encoder layer
        transformer_enc_positional_dropout_rate: 0.1     # dropout rate for transformer encoder positional encoding
        transformer_enc_attn_dropout_rate: 0.1           # dropout rate for transformer encoder attention layer
        transformer_dec_dropout_rate: 0.1                # dropout rate for transformer decoder layer
        transformer_dec_positional_dropout_rate: 0.1     # dropout rate for transformer decoder positional encoding
        transformer_dec_attn_dropout_rate: 0.1           # dropout rate for transformer decoder attention layer
        transformer_enc_dec_attn_dropout_rate: 0.1       # dropout rate for transformer encoder-decoder attention layer
        use_guided_attn_loss: true                       # whether to use guided attention loss
        num_heads_applied_guided_attn: 2                 # number of layers to apply guided attention loss
        num_layers_applied_guided_attn: 2                # number of heads to apply guided attention loss
        modules_applied_guided_attn: ["encoder-decoder"] # modules to apply guided attention loss
        guided_attn_loss_sigma: 0.4                      # sigma in guided attention loss
        guided_attn_loss_lambda: 10.0                    # lambda in guided attention loss

    # copied from pretrained vocoder's config.yaml
    vocoder_input_type: gt
    vocoder_type: nsf_generator
    vocoder_params:
        in_dim: 128
        out_dim: 1
        prj_conf_input_reso: 288
        prj_conf_wav_samp_rate: 24000

    # discriminator
    use_discriminator: false

    # loss function related
    generator_adv_loss_type: nsf_generator_loss
    generator_adv_loss_params:
        frame_hops:
        - 80
        - 40
        - 640
        frame_lens:
        - 320
        - 80
        - 1920
        fft_n:
        - 4096
        - 4096
        - 4096
        amp_floor: 0.0001
    use_feat_match_loss: true            # whether to use feat match loss
    feat_match_loss_params:
        average_by_discriminators: false # whether to average loss value by #discriminators
        average_by_layers: false         # whether to average loss value by #layers of each discriminator
        include_final_outputs: true      # whether to include final outputs for loss calculation
    use_mel_loss: true     # whether to use mel-spectrogram loss
    mel_loss_params:
        fs: 24000          # must be the same as the training data
        n_fft: 1024        # fft points
        hop_length: 256    # hop size
        win_length: null   # window length
        window: hann       # window type
        n_mels: 80         # number of Mel basis
        fmin: 0            # minimum frequency for Mel basis
        fmax: null         # maximum frequency for Mel basis
        log_base: null     # null represent natural log
    lambda_text2mel: 0.0   # loss scaling coefficient for text2mel loss
    lambda_adv: 1.0        # loss scaling coefficient for adversarial loss
    lambda_mel: 45.0       # loss scaling coefficient for Mel loss
    lambda_feat_match: 2.0 # loss scaling coefficient for feat match loss

    # others
    sampling_rate: 24000           # needed in the inference for saving wav
    segment_size: 32               # segment size for random windowed discriminator
    cache_generator_outputs: false # whether to cache generator outputs in the training # NOTE (Xuan): set as false becuase it does not use discriminator

# initialization (might need to modify for your own pretrained model)
init_param:
- exp/tts_train_raw_proll/train.loss.ave_5best.pth:tts:tts.generator.text2mel      # path:src_key:dst_key
# - exp/ljspeech_hifigan.v1/generator.pth::tts.generator.vocoder
# - exp/ljspeech_hifigan.v1/discriminator.pth::tts.discriminator
freeze_param:
- tts.generator.text2mel

##########################################################
#            OPTIMIZER & SCHEDULER SETTING               #
##########################################################
# optimizer setting for generator
optim: adamw
optim_conf:
    lr: 2.0e-4
    betas: [0.8, 0.99]
    eps: 1.0e-9
    weight_decay: 0.0
scheduler: exponentiallr
scheduler_conf:
    gamma: 0.999875
# optimizer setting for discriminator
optim2: adamw
optim2_conf:
    lr: 2.0e-4
    betas: [0.8, 0.99]
    eps: 1.0e-9
    weight_decay: 0.0
scheduler2: exponentiallr
scheduler2_conf:
    gamma: 0.999875
generator_first: true # whether to start updating generator first

##########################################################
#                OTHER TRAINING SETTING                  #
##########################################################
num_iters_per_epoch: 1000 # number of iterations per epoch
max_epoch: 1000           # number of epochs
accum_grad: 1             # gradient accumulation
batch_bins: 5000000       # batch bins (feats_type=raw) (TODO: usually 5000000)
batch_type: numel         # how to make batch
grad_clip: -1             # gradient clipping norm
grad_noise: false         # whether to use gradient noise injection
sort_in_batch: descending # how to sort data in making batch
sort_batch: descending    # how to sort created batches
num_workers: 4            # number of workers of data loader
use_amp: false            # whether to use pytorch amp
log_interval: 50          # log interval in iterations
keep_nbest_models: 5      # number of models to keep
num_att_plot: 3           # number of attention figures to be saved in every check
seed: 777                 # random seed number
patience: null            # patience for early stopping
unused_parameters: true   # needed for multi gpu case
best_model_criterion:     # criterion to save the best models
-   - valid
    - text2mel_loss
    - min
-   - train
    - text2mel_loss
    - min
-   - train
    - total_count
    - max
cudnn_deterministic: false # setting to false accelerates the training speed but makes it non-deterministic
                           # in the case of GAN-TTS training, we strongly recommend setting to false
cudnn_benchmark: false     # setting to true might acdelerate the training speed but sometimes decrease it
                           # therefore, we set to false as a default (recommend trying both cases)
turns:
- generator
